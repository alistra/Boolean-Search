\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[]{polski}

\newcommand{\handout}[5]{
  \noindent
  \begin{center}
  \framebox{
    \vbox{
      \hbox to 5.78in { {\bf Wyszukiwanie informacji} \hfill #2 }
      \vspace{4mm}
      \hbox to 5.78in { {\Large \hfill #5  \hfill} }
      \vspace{2mm}
      \hbox to 5.78in { {\em #3 \hfill #4} }
    }
  }
  \end{center}
  \vspace*{4mm}
}

\newcommand{\lecture}[4]{\handout{#1}{#2}{#3}{#4}{#1}}
\newcommand{\high}{\mathrm{high}}
\newcommand{\low}{\mathrm{low}}

\newtheorem{theorem}{Twierdzenie}
\newtheorem{corollary}[theorem]{Wniosek}
\newtheorem{lemma}[theorem]{Lemat}
\newtheorem{observation}[theorem]{Obserwacja}
\newtheorem{definition}[theorem]{Definicja}
\newtheorem{fact}[theorem]{Fakt}
\newtheorem{assumption}[theorem]{Założenie}

% 1-inch margins, from fullpage.sty by H.Partl, Version 2, Dec. 15, 1988.
\topmargin 0pt
\advance \topmargin by -\headheight
\advance \topmargin by -\headsep
\textheight 8.9in
\oddsidemargin 0pt
\evensidemargin \oddsidemargin
\marginparwidth 0.5in
\textwidth 6.5in

\parindent 0in
\parskip 1.5ex
%\renewcommand{\baselinestretch}{1.25}

\begin{document}

\lecture{Dokumentacja wyszukiwarki boolowskiej}{Lato 2011}{Tomasz Jurdzinski}{Aleksander Balicki, Tomasz Maciejewski}

\section{Instalacja}
Pliki projektu wgrywamy do jednego folderu.\\
Do podfolderu \texttt{data/} wgrywamy pliki źródłowe morfologika i wikipedii.
\section{Użytkowanie}
<DOROBIC FAJNE UI>
\section{Opis użytych algorytmów i struktur danych}
\subsection{Tworzenie indeksu}
Proces tworzenia wykorzystuje ideę MapReduce.
\subsubsection{Faza map}
Na początku przechodzimy przez plik wikipedii po linii i wyrażeniem regularnym wyznaczamy słowa. 
Dla każdej znormalizowanej formy słowa. tworzymy parę $(słowo,\;nr_dokumentu,\;pozycja)$ i
dodajemy ją do pliku tymczasowego \texttt{WORDS} jako jedną linię.
\subsubsection{Faza reduce}
Po przejściu przez cały plik wikipedii sortujemy plik \texttt{WORDS}, stabilnie, po pierwszym słowie,
tym sposobem mamy zachowaną kolejność wystąpień dokumentów i pozycji w ramach artykułu.
Przechodzimy teraz przez posortowany plik \texttt{WORDS.sorted}, i dla każdego trójliterowego prefiksu (lub krótszego,
jeżeli całe słowo jest krótsze niż 3 litery) tworzymy tablice hashującą z listą postingową (odpowiednio skompresowaną lub nie).
Zapisujemy tą tablice do pliku z użyciem biblioteki do serializacji.
W ten sam sposób najpierw serializujemy morfologika, aby potem móc szybko normalizować słowa.
\subsection{Wyszukiwanie}
Sposób wyszukiwania interaktywnego, to szczególny przypadek wyszukiwania w formie wsadowej.
Wyszukiwarka w formie wsadowej, po wczytaniu wszystkich zapytań, gromadzi z nich słowa, grupując po prefiksie (hash z prefixami jako klucze i pythonowymi setami słów).
\subsection{Normalizacja}
Dla słowa $w$, odczytujemy plik zależny od trójliterowego prefiksu $w$ i sprawdzamy, czy słowo jest w tym słowniku,
jeśli tak zwracamy wszystkie jego formy bazowe, odpowiednio po operacji stemmingu lub nie.
\subsection{Spamiętywanie}
\subsection{Struktury danych}
\begin{enumerate}
\item dict() - pythonowa wbudowana tablica hashująca
\item set() - pythonowa wbudowana implementacja zbioru, też bazowana na tablicy hashującej
\end{enumerate}
\section{Opis użytych bibliotek}
\subsection{marshal}
\subsection{gzip}
\subsection{pstats}
\subsection{unittest}
\section{Opis testów}
\end{document}

