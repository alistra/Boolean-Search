\documentclass[11pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{polski}
\usepackage{a4wide}

\author{Aleksander Balicki, Tomasz Maciejewski}
\title{Dokumentacja wyszukiwarki zapytań boolowskich}

\begin{document}

\maketitle

\section{Wstęp}
Dokument opisuje proces instalacji i uruchamiania wyszukiwarki zapytań boolowskich, oraz użyte biblioteki i struktury danych.

\section{Instalacja}
Pliki projektu wgrywamy do jednego folderu. Do podfolderu \texttt{data} wgrywamy pliki:
\begin{enumerate}
\item \texttt{morfologik\_do\_wyszukiwarek.txt} -- dane do morfologika,
\item \texttt{wikipedia\_dla\_wyszukiwarek.txt} -- plik z danymi źródłowymi
\end{enumerate}

\section{Użytkowanie}
Interfejs wyszukiwarki uruchamia się poleceniem \texttt{boolsearch.py}. Domyślnym trybem jest tryb wsadowy, który będzie czytał zapytania ze standardowego wejścia aż do napotkania znaku końca pliku.

Tryb interaktywny wywołuje się poleceniem \texttt{boolsearch.py i}. W trybie interaktywnym wczytywane są zapytania również ze standardowego wejścia, ale wyniki wypisywane są od razu po wczytaniu zapytania.

\section{Opis użytych algorytmów i struktur danych}

\subsection{Tworzenie indeksu}
Na początku wczytujemy dane do morfologika, tworząc z nich słownik. Następnie proces tworzenia indeksu przebiega w kilku fazach. Indeks jest tworzony w podkatalogu \texttt{index}.

\subsubsection{Faza zbierania informacji}
Przechodzimy przez plik z danymi wiersz po wierszu, zapamiętując numer dokumentu, którego wiersze analizujemy. Numery i tytuły dokumentów zapisywane są do pliku \texttt{TITLES}. Wiersz rozbijamy na słowa, pamiętając pozycję słowa w dokumencie, a ze słów -- przy pomocy morfologika -- tworzymy jego znormalizowane formy, które odpowiednio stemmujemy lub nie. Stemming polega na odcięciu jednej z wyliczonych końcówek. Dla każdej znormalizowanej formy słowa zapisujemy do pliku \texttt{WORDS} wiersz: słowo, numer dokumentu, pozycja słowa w dokumencie.

\subsubsection{Faza sortowania}
Po przejściu przez cały plik z danymi sortujemy stabilnie plik \texttt{WORDS} po pierwszym elemencie, czyli po znormalizowanej formie słowa. Stabilność zagwarantuje nam dobrą kolejność numerów dokumentów i pozycji w dokumencie. Wynik sortowania zapisujemy w pliku \texttt{WORDS.sorted}.

Do sortowania używany jest UNIX-owy program \texttt{sort}, uruchamiany w następujący sposób:
\begin{verbatim}
LC_ALL=C sort -T. -k1,1 -s WORDS > WORDS.sorted
\end{verbatim}

\subsubsection{Faza tworzenia indeksu}
Następnie przechodzimy przez posortowany plik \texttt{WORDS.sorted} i parsując każdy wiersz, dodajemy go do odpowiedniego słownika. Parsowanie polega po prostu na odczytaniu trzech pól wiersza: słowa, dokumentu i pozycji.

Słownik jest indeksem wszystkich słów, których pierwsze pięć (lub odpowiednio ustawiona liczba) liter jest taka sama. Słowniki te kolejno zapisujemy na dysk przy pomocy biblioteki \texttt{marshal} służącej do serializacji obiektów Pythona i odpowiednio gzipujemy i kodujemy różnicowo, jeżeli opcja kopresji jest włączona. Dzięki sortowaniu każdy słownik będziemy otwierali do zapisu tylko raz, aż się się skończy jego prefiks. Zapisujemy dla każdego prefiksu wersje słownika pozycyjną i niepozycyjną.

\subsubsection{Faza tworzenia indeksu morfologika}
W ten sam sposób sortujemy plik z danymi morfologika i indeksujemy do pięcioliterowych słowników, aby potem móc szybko normalizować słowa.

\subsection{Wyszukiwanie}
Wyszukiwarka w formie wsadowej po wczytaniu wszystkich zapytań gromadzi z nich słowa, grupując po 3-literowym prefiksie. Dla każdego prefiksu jest otwierany plik odpowiadający za słowa z tym prefiksem. Z tego pliku dodawane są szukane słowa do słownika w pamięci. Tak samo obsługiwane jest pobieranie informacji z morfologika.

Po wczytaniu wszystkich potrzebnych postingów i danych z morfologika, zapytania zostają sparsowane i przeprowadzane są odpowiednie scalania list postingowych zgodnie z rozwiązaniami przedstawionymi na ćwiczeniach. Negacja jest przenoszona w górę drzewa zapytania. Jeśli wynik końcowy dla zapytania wyszedł z negacją, to odejmowany jest wynik od listy wszystkich dokumentów.

Po otrzymaniu wyników dla wszystkich zapytań jest wczytywany plik z tytułami i wypisywane są tytuły dla dokumentów wynikowych.

\subsection{Struktury danych}
\begin{enumerate}
\item dict() - pythonowa wbudowana tablica hashująca
\item set() - pythonowa wbudowana implementacja zbioru, też bazująca na tablicy hashującej
\item list() - pythonowa wbudowana implementacja listy
\end{enumerate}

\section{Opis użytych bibliotek}

\subsection{marshal}
Jest to biblioteka do serializacji obiektów pythonowych, według testów najszybsza z dostępnych.

Zapisujemy obiekt poleceniem:
\begin{verbatim}
marshal.dump(obiekt, uchwyt_do_pliku)
\end{verbatim}
Odczytujemy:
\begin{verbatim}
obiekt = marshal.load(uchwyt_do_pliku)
\end{verbatim}

Używana jest do serializacji słowników przechowujących postingi i dane z morfologika oraz listy tytułów.

\subsection{gzip}
Jest to biblioteka do zapisywania i odczytywania plików skompresowanych programem \texttt{gzip}.

Używana jest specjalna funkcja do otwarcia pliku, która zwraca nam uchwyt, którego można używać do pisania lub czytania:
\begin{verbatim}
handle = gzip.open(filename, 'wb') 
handle.write("text")
\end{verbatim}

\section{Opis testów}
Zostały wykorzystane testy podane na KNO oraz komputer z procesorem Intel Core 2 Duo 2.4 GHz. Uzyskano następujące czasy:
\begin{center}
\begin{tabular}{|c|c|}
\hline
Nazwa & Czas \\ \hline
pytania\_and\_dla\_IR.txt & 6m48s \\ \hline
pytania\_or\_dla\_IR.txt & 12m06s \\ \hline
pytania\_frazowe.txt & 19m27s \\ \hline
\end{tabular}
\end{center}

Sprawdziliśmy też według zaleceń, czy wszystkie zapytania mają niepustą odpowiedź, w pytaniach dla \texttt{and} i \texttt{or}.

\end{document}
